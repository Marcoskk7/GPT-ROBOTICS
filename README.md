# LLM-Enhanced Robotic Manipulation System

ğŸ¤– **An Intelligent Robotics System Integrating Large Language Models with Panda Robot for Complex Manipulation Tasks**

## ğŸ¯ Project Overview

Our project is designed to utilize the Panda robot for intelligent path planning to solve two primary manipulation problems:

1. **Cup Placement Task**: Precisely placing an empty cup on a coaster
2. **Block Stacking Task**: Systematically stacking three colored blocks in a specific sequence

The system features **randomly generated themes**, requiring the robot to intelligently determine correct target locations and complete tasks autonomously. The highlight of our project lies in the innovative integration of **Large Language Model (LLM) systems**, enabling users to control the robot through natural language input in both English and Chinese.

## ğŸš€ Key Features

### ğŸ§  Intelligent Planning Methods
- **Sampling-based Algorithms**: Utilizing the [Open Motion Planning Library (OMPL)](https://ompl.kavrakilab.org/) for robust path planning
- **Screw Motion Planning**: Advanced trajectory generation through:
  - Relative transformation calculation from current to target pose
  - Exponential coordinates computation of relative transformations
  - Joint velocity calculation using Jacobian matrix methods

### ğŸ—£ï¸ Natural Language Interface
- **Multi-language Support**: Natural English and Chinese command processing
- **LLM Integration**: DeepSeek API integration with local fallback
- **Intuitive Control**: Users can command the robot using everyday language
- **Smart Parsing**: Advanced command interpretation with confidence scoring

### ğŸ® Dual Task Capabilities
- **Cup Placement Environment**: Precise manipulation for tableware organization
- **Block Stacking Environment**: Complex multi-object manipulation with spatial reasoning

## ğŸ“ Project Structure

```
copy_Robotwin/
â”œâ”€â”€ envs/
â”‚   â”œâ”€â”€ llm_enhanced_stack.py      # LLM-integrated block stacking system
â”‚   â”œâ”€â”€ demo_stack.py              # Core block stacking implementation
â”‚   â”œâ”€â”€ llm_interface.py           # LLM communication interface
â”‚   â”œâ”€â”€ camera.py                  # Camera and vision systems
â”‚   â”œâ”€â”€ robot.py                   # Robot control and kinematics
â”‚   â””â”€â”€ utils.py                   # Utility functions
â”œâ”€â”€ config/
â”‚   â””â”€â”€ llm_config.py              # LLM and system configuration
â”œâ”€â”€ assets/                        # 3D models and textures
â”œâ”€â”€ requirements.txt               # Dependencies
â””â”€â”€ README.md                      # This file
```

## ğŸ› ï¸ Installation

### Prerequisites
- Python 3.10+
- CUDA-capable GPU (recommended)
- Linux/Ubuntu 22.04+ (recommended)

### Step 1: Environment Setup
```bash
# Create conda environment
conda create -n robotwin_env python=3.9
conda activate robotwin_env

# Clone repository
git clone <your-repo-url>
cd copy_Robotwin
```

### Step 2: Install Dependencies
```bash
# Install PyTorch with CUDA support
pip install torch torchvision --index-url https://download.pytorch.org/whl/cu121

# Install SAPIEN simulation framework
pip install sapien

# Install remaining dependencies
pip install -r requirements.txt
```

### Step 3: Install Motion Planning Library
```bash
# Install MPLib for motion planning
pip install mplib
```

## âš™ï¸ Configuration

### LLM Setup (Optional but Recommended)
1. **DeepSeek API Configuration**:
   ```python
   # In config/llm_config.py
   DEFAULT_LLM_CONFIG = {
       "type": "deepseek",
       "api_key": "your-deepseek-api-key-here",  # Replace with actual key
       "model": "deepseek-chat",
       "base_url": "https://api.deepseek.com"
   }
   ```

2. **Local Mode**: If no API key is provided, the system automatically falls back to local keyword matching.

## ğŸ® Usage

### Block Stacking Task with LLM Integration

```bash
cd copy_Robotwin
python -c "
from envs.llm_enhanced_stack import LLMEnhancedStackEnvironment

# Initialize environment
env = LLMEnhancedStackEnvironment()
env.create_environment()

# Start interactive mode
env.interactive_mode()
"
```

### Interactive Commands

Once in interactive mode, you can use natural language commands:

#### ğŸ§± Block Stacking Commands
```
ğŸ¯ Natural Language Examples:
â€¢ "Stack all blocks"
â€¢ "å åŠ æ‰€æœ‰æ–¹å—"
â€¢ "Pile up the blocks"
â€¢ "å°†æ–¹å—å †èµ·æ¥"
```

#### ğŸ”§ Robot Control Commands
```
â€¢ "Open gripper" / "æ‰“å¼€å¤¹çˆª"
â€¢ "Close gripper" / "å…³é—­å¤¹çˆª"  
â€¢ "Go home" / "å›åˆ°åˆå§‹ä½ç½®"
â€¢ "Show status" / "æ˜¾ç¤ºçŠ¶æ€"
â€¢ "Reset scene" / "é‡ç½®åœºæ™¯"
```

#### ğŸ“‹ System Commands
```
â€¢ help/h - Show help information
â€¢ status/s - Display environment status
â€¢ history - Show task execution history
â€¢ clear - Clear screen
â€¢ quit/q - Exit program
```

### Example Session
```
ğŸ¤– æ¬¢è¿ä½¿ç”¨LLMå¢å¼ºçš„æ–¹å—å åŠ ç³»ç»Ÿï¼

ğŸ¯ è¯·è¾“å…¥æŒ‡ä»¤> Stack all the blocks
ğŸ“ ç”¨æˆ·è¾“å…¥: Stack all the blocks
ğŸ” è§£æç»“æœ: æŒ‰é»˜è®¤é¡ºåºå åŠ æ‰€æœ‰æ–¹å— (ä¿¡å¿ƒåº¦: 0.90)
ğŸ¤– æ­£åœ¨æ‰§è¡Œä»»åŠ¡...
å¼€å§‹è§„åˆ’æ–¹å—å åŠ ä»»åŠ¡...
=== é˜¶æ®µ1ï¼šå°†æ–¹å—2å åŠ åˆ°æ–¹å—1ä¸Š ===
æ­¥éª¤1: ç§»åŠ¨åˆ°æ–¹å—2ä¸Šæ–¹...
... (detailed execution steps)
âœ… ä»»åŠ¡æ‰§è¡ŒæˆåŠŸ!
```

## ğŸ—ï¸ System Architecture

### Core Components

1. **LLMEnhancedStackEnvironment**: Main system integrating LLM with robotics
2. **Motion Planning Pipeline**:
   - OMPL-based sampling algorithms
   - Screw motion trajectory generation
   - Collision detection and avoidance
3. **Natural Language Processing**:
   - Command parsing and intent recognition
   - Confidence scoring and validation
   - Multi-language support

### Task Execution Flow

```mermaid
graph TD
    A[User Input] --> B[LLM Processing]
    B --> C[Command Parsing]
    C --> D[Intent Recognition]
    D --> E[Motion Planning]
    E --> F[Path Execution]
    F --> G[Task Completion]
    G --> H[Status Feedback]
```

## ğŸ¨ Supported Tasks

### 1. Block Stacking Task
- **Objective**: Stack three colored blocks (Redâ†’Greenâ†’Blue)
- **Complexity**: Multi-stage manipulation with precise positioning
- **Features**: 
  - Automatic block detection
  - Collision-free path planning
  - Stable stacking sequences

### 2. Cup Placement Task (Future Enhancement)
- **Objective**: Place cups on designated coasters
- **Features**: Precision placement with orientation control

## ğŸ”§ Technical Details

### Motion Planning Algorithms
1. **OMPL Integration**: Utilizes RRT*, PRM, and other sampling-based algorithms
2. **Screw Motion Theory**: Implements SE(3) motion planning with:
   - Lie algebra exponential mapping
   - Jacobian-based velocity control
   - Smooth trajectory interpolation

### LLM Integration
- **Primary**: DeepSeek API for advanced natural language understanding
- **Fallback**: Local keyword matching for offline operation
- **Confidence Scoring**: Ensures reliable command interpretation

## ğŸ› Troubleshooting

### Common Issues

1. **SAPIEN Installation Errors**:
   ```bash
   # Try installing with specific version
   pip install sapien==3.0.0b1
   ```

2. **CUDA/GPU Issues**:
   ```bash
   # Install CPU-only PyTorch if GPU unavailable
   pip install torch torchvision --index-url https://download.pytorch.org/whl/cpu
   ```

3. **Motion Planning Failures**:
   - Check joint limits and workspace boundaries
   - Verify collision meshes are properly loaded
   - Increase planning timeout in configuration

## ğŸ¤ Contributing

We welcome contributions! Please follow these steps:

1. Fork the repository
2. Create a feature branch (`git checkout -b feature/amazing-feature`)
3. Commit your changes (`git commit -m 'Add amazing feature'`)
4. Push to the branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **SAPIEN Simulation Platform**: For providing the physics simulation environment
- **MPLib**: For motion planning algorithms and implementations
- **DeepSeek**: For LLM API services
- **Open Motion Planning Library (OMPL)**: For sampling-based planning algorithms

## ğŸ“ Contact

For questions, issues, or collaborations, please contact:
- **Project Lead**: [Marcos] - [marcus20220122@gmail.com]
- **Issues**: [GitHub Issues](link-to-issues)
- **Discussions**: [GitHub Discussions](link-to-discussions)

---

ğŸš€ **Ready to explore intelligent robotics with natural language control? Get started now!**
